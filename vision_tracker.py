import cv2
import numpy as np
from ultralytics import YOLO
from picamera2 import Picamera2
from config import *
import time

class VisionTracker:
    """è§†è§‰è¿½è¸ªå™¨ - è´Ÿè´£ç›®æ ‡æ£€æµ‹ã€è¯†åˆ«å’Œè¿½è¸ª"""
    
    def __init__(self):
        # åˆå§‹åŒ–æ‘„åƒå¤´
        self.camera = Picamera2()
        config = self.camera.create_preview_configuration(
            main={"size": (CAMERA_WIDTH, CAMERA_HEIGHT), "format": "RGB888"}
        )
        self.camera.configure(config)
        self.camera.start()
        time.sleep(2)  # ç­‰å¾…æ‘„åƒå¤´é¢„çƒ­
        print("âœ“ Camera initialized")
        
        # åˆå§‹åŒ–YOLO
        self.yolo = YOLO(YOLO_MODEL)
        print(f"âœ“ YOLO model loaded: {YOLO_MODEL}")
        
        # ç›®æ ‡ç‰¹å¾å­˜å‚¨
        self.target_upper_color = None  # ä¸Šèº«é¢œè‰²ç›´æ–¹å›¾
        self.target_lower_color = None  # ä¸‹èº«é¢œè‰²ç›´æ–¹å›¾
        self.target_captured = False
        
        # è¿½è¸ªçŠ¶æ€
        self.lost_frame_count = 0
        
    def capture_frame(self):
        """æ•è·ä¸€å¸§å›¾åƒ"""
        frame = self.camera.capture_array()
        return frame
    
    def detect_people(self, frame):
        """
        ä½¿ç”¨YOLOæ£€æµ‹ç”»é¢ä¸­çš„æ‰€æœ‰äºº
        
        è¿”å›:
            detections: list of dict, æ¯ä¸ªåŒ…å« {bbox, confidence}
                bbox = (x, y, w, h)
        """
        results = self.yolo(frame, conf=YOLO_CONFIDENCE, verbose=False)
        
        people = []
        for result in results:
            boxes = result.boxes
            for box in boxes:
                # åªä¿ç•™"äºº"ç±»åˆ« (class_id = 0)
                if int(box.cls[0]) == 0:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    w = x2 - x1
                    h = y2 - y1
                    conf = float(box.conf[0])
                    
                    people.append({
                        'bbox': (int(x1), int(y1), int(w), int(h)),
                        'confidence': conf
                    })
        
        return people
    
    def extract_clothing_color(self, frame, bbox):
        """
        æå–äººç‰©çš„ä¸Šä¸‹èº«é¢œè‰²ç‰¹å¾
        
        å‚æ•°:
            frame: å›¾åƒ
            bbox: (x, y, w, h)
        
        è¿”å›:
            upper_hist: ä¸Šèº«é¢œè‰²ç›´æ–¹å›¾
            lower_hist: ä¸‹èº«é¢œè‰²ç›´æ–¹å›¾
        """
        x, y, w, h = bbox
        
        # æå–äººç‰©åŒºåŸŸ
        person_roi = frame[y:y+h, x:x+w]
        
        # åˆ†å‰²ä¸Šä¸‹èº«
        upper_h = int(h * UPPER_BODY_RATIO)
        upper_roi = person_roi[:upper_h, :]
        lower_roi = person_roi[upper_h:, :]
        
        # è½¬æ¢åˆ°HSVç©ºé—´ (å¯¹å…‰ç…§å˜åŒ–æ›´é²æ£’)
        upper_hsv = cv2.cvtColor(upper_roi, cv2.COLOR_RGB2HSV)
        lower_hsv = cv2.cvtColor(lower_roi, cv2.COLOR_RGB2HSV)
        
        # è®¡ç®—é¢œè‰²ç›´æ–¹å›¾ (Hé€šé“)
        upper_hist = cv2.calcHist([upper_hsv], [0], None, [32], [0, 180])
        lower_hist = cv2.calcHist([lower_hsv], [0], None, [32], [0, 180])
        
        # å½’ä¸€åŒ–
        cv2.normalize(upper_hist, upper_hist, 0, 1, cv2.NORM_MINMAX)
        cv2.normalize(lower_hist, lower_hist, 0, 1, cv2.NORM_MINMAX)
        
        return upper_hist, lower_hist
    
    def compare_clothing(self, upper_hist, lower_hist):
        """
        æ¯”è¾ƒé¢œè‰²ç›´æ–¹å›¾ç›¸ä¼¼åº¦
        
        è¿”å›:
            similarity: 0-1, è¶Šé«˜è¶Šç›¸ä¼¼
        """
        if self.target_upper_color is None:
            return 0.0
        
        # ä½¿ç”¨ç›¸å…³æ€§æ¯”è¾ƒç›´æ–¹å›¾
        upper_sim = cv2.compareHist(
            upper_hist, 
            self.target_upper_color, 
            cv2.HISTCMP_CORREL
        )
        lower_sim = cv2.compareHist(
            lower_hist, 
            self.target_lower_color, 
            cv2.HISTCMP_CORREL
        )
        
        # åŠ æƒå¹³å‡ (ä¸Šèº«æƒé‡æ›´é«˜,å› ä¸ºæ›´ç¨³å®š)
        similarity = 0.6 * upper_sim + 0.4 * lower_sim
        
        return similarity
    
    def capture_target_features(self):
        """
        åˆå§‹åŒ–é˜¶æ®µ: æ‹æ‘„å¤šå¼ ç…§ç‰‡,è®°å½•ç›®æ ‡ç‰¹å¾
        
        è¿”å›:
            success: bool
        """
        print(f"\nğŸ“¸ Capturing target features ({NUM_INIT_SAMPLES} samples)...")
        
        upper_hists = []
        lower_hists = []
        
        for i in range(NUM_INIT_SAMPLES):
            print(f"  Sample {i+1}/{NUM_INIT_SAMPLES}...", end='')
            
            # æ‹æ‘„ä¸€å¸§
            frame = self.capture_frame()
            
            # æ£€æµ‹äºº
            people = self.detect_people(frame)
            
            if len(people) == 0:
                print(" âœ— No person detected!")
                time.sleep(1)
                continue
            
            # é€‰æ‹©æœ€å¤§çš„äººæ¡† (å‡è®¾æ˜¯ç›®æ ‡)
            largest = max(people, key=lambda p: p['bbox'][2] * p['bbox'][3])
            
            # æå–é¢œè‰²ç‰¹å¾
            upper_hist, lower_hist = self.extract_clothing_color(
                frame, largest['bbox']
            )
            
            upper_hists.append(upper_hist)
            lower_hists.append(lower_hist)
            
            print(" âœ“")
            time.sleep(0.5)
        
        if len(upper_hists) < 2:
            print("âœ— Failed to capture enough samples!")
            return False
        
        # å–ä¸­ä½æ•°ä½œä¸ºç›®æ ‡ç‰¹å¾ (å»é™¤å¼‚å¸¸å€¼)
        self.target_upper_color = np.median(upper_hists, axis=0)
        self.target_lower_color = np.median(lower_hists, axis=0)
        self.target_captured = True
        
        print("âœ“ Target features captured!")
        return True
    
    def find_target(self, frame):
        """
        åœ¨ç”»é¢ä¸­å¯»æ‰¾ç›®æ ‡äººç‰©
        
        è¿”å›:
            target: dict or None
                {
                    'bbox': (x, y, w, h),
                    'center': (cx, cy),
                    'area': int,
                    'similarity': float
                }
        """
        if not self.target_captured:
            return None
        
        # æ£€æµ‹æ‰€æœ‰äºº
        people = self.detect_people(frame)
        
        if len(people) == 0:
            self.lost_frame_count += 1
            return None
        
        # ä¸ç›®æ ‡åŒ¹é…
        best_match = None
        best_similarity = 0.0
        
        for person in people:
            bbox = person['bbox']
            
            # æå–é¢œè‰²ç‰¹å¾
            upper_hist, lower_hist = self.extract_clothing_color(frame, bbox)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.compare_clothing(upper_hist, lower_hist)
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = person
        
        # æ£€æŸ¥æ˜¯å¦è¶³å¤Ÿç›¸ä¼¼
        if best_similarity < COLOR_MATCH_THRESHOLD:
            self.lost_frame_count += 1
            return None
        
        # æ‰¾åˆ°ç›®æ ‡!
        self.lost_frame_count = 0
        
        bbox = best_match['bbox']
        x, y, w, h = bbox
        
        return {
            'bbox': bbox,
            'center': (x + w//2, y + h//2),
            'area': w * h,
            'similarity': best_similarity
        }
    
    def estimate_distance_from_bbox(self, bbox_area):
        """
        æ ¹æ®äººæ¡†é¢ç§¯ä¼°ç®—è·ç¦»
        
        è¿™éœ€è¦é¢„å…ˆæ ‡å®š! è¿™é‡Œç”¨ç®€å•çš„åæ¯”ä¾‹å…³ç³»
        
        å‚æ•°:
            bbox_area: äººæ¡†é¢ç§¯ (åƒç´ Â²)
        
        è¿”å›:
            distance: ä¼°ç®—è·ç¦» (cm)
        """
        # ç®€åŒ–æ¨¡å‹: distance âˆ 1/âˆšarea
        # å‡è®¾åœ¨100cmæ—¶,bboxé¢ç§¯çº¦ä¸º15000åƒç´ Â²
        REFERENCE_AREA = 15000
        REFERENCE_DISTANCE = 100
        
        if bbox_area < 1000:  # é˜²æ­¢é™¤é›¶
            return 300
        
        estimated_dist = REFERENCE_DISTANCE * np.sqrt(REFERENCE_AREA / bbox_area)
        
        return estimated_dist
    
    def draw_debug_info(self, frame, target, offset_x, steering):
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶è°ƒè¯•ä¿¡æ¯
        
        å‚æ•°:
            frame: å›¾åƒ
            target: ç›®æ ‡ä¿¡æ¯ (æˆ–None)
            offset_x: æ°´å¹³åç§»
            steering: è½¬å‘é‡
        """
        debug_frame = frame.copy()
        
        # ç”»ä¸­å¿ƒçº¿
        cv2.line(debug_frame, 
                 (CAMERA_WIDTH//2, 0), 
                 (CAMERA_WIDTH//2, CAMERA_HEIGHT),
                 (0, 255, 0), 2)
        
        # ç”»æ­»åŒº
        deadzone_left = CAMERA_WIDTH//2 - CENTER_DEADZONE
        deadzone_right = CAMERA_WIDTH//2 + CENTER_DEADZONE
        cv2.rectangle(debug_frame,
                      (deadzone_left, 0),
                      (deadzone_right, CAMERA_HEIGHT),
                      (0, 255, 0), 1)
        
        if target:
            # ç”»äººæ¡†
            x, y, w, h = target['bbox']
            cv2.rectangle(debug_frame, (x, y), (x+w, y+h), (0, 255, 255), 2)
            
            # ç”»ä¸­å¿ƒç‚¹
            cx, cy = target['center']
            cv2.circle(debug_frame, (cx, cy), 5, (255, 0, 0), -1)
            
            # ç”»è¿çº¿
            cv2.line(debug_frame,
                     (CAMERA_WIDTH//2, CAMERA_HEIGHT//2),
                     (cx, cy),
                     (255, 0, 0), 2)
            
            # æ˜¾ç¤ºä¿¡æ¯
            info_text = f"Sim: {target['similarity']:.2f} | Area: {target['area']}"
            cv2.putText(debug_frame, info_text, (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            offset_text = f"Offset: {offset_x:.0f}px | Steer: {steering:.2f}"
            cv2.putText(debug_frame, offset_text, (10, 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        else:
            cv2.putText(debug_frame, "TARGET LOST", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        
        return debug_frame
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.camera.stop()
        print("Camera stopped")